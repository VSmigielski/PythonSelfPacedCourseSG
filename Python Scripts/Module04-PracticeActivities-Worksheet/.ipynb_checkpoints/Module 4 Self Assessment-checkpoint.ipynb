{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment walks you through the steps required to perform an advanced frequency analysis on words in a given text source. It includes the following steps:\n",
    "\n",
    "Convert a text file into a string.\n",
    "Split a string into words, excluding punctuation marks.\n",
    "Remove stop words from the string.\n",
    "Lemmatize the words in the string so that all words are stem words.\n",
    "Count the frequency of each stem word and store the results in a dictionary.\n",
    "Convert the dictionary to a JSON file.\n",
    "You may use any text file you wish, including files used in lessons and exercises in this course, files downloaded from a website like Project Gutenberg (Links to an external site.), or a file you create specifically for this assignment. After completing the activity, you should test it using at least one other file.\n",
    "\n",
    "You may create this as a single script that includes all steps, or you can split the steps into individual scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Convert a Text File to a String\n",
    "Create a function that takes as input the path to a text file and returns the contents of the file as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n",
      "Hello World!\n",
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "def read_text_file(file_path):\n",
    "    f = open(file_path, \"r\")\n",
    "    text = f.read()\n",
    "    return text\n",
    "\n",
    "text = read_text_file(\"FileIO-DataFiles/test_file.txt\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Split the String into Words\n",
    "Create a function that takes as input a string and returns a list of strings representing the words in the text file.\n",
    "\n",
    "The function should divide the string into words based on any type of punctuation.\n",
    "\n",
    "The function should convert all words into lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flatland', 'part', '1', 'this', 'world', 'section', '1', 'of', 'the', 'nature', 'of', 'flatland', 'i', 'call', 'our', 'world', 'flatland', 'not', 'because', 'we', 'call', 'it', 'so', 'but', 'to', 'make', 'its', 'nature', 'clearer', 'to', 'you', 'my', 'happy', 'readers', 'who', 'are', 'privileged', 'to', 'live', 'in', 'space', 'imagine', 'a', 'vast', 'sheet', 'of', 'paper', 'on', 'which', 'straight', 'lines', 'triangles', 'squares', 'pentagons', 'hexagons', 'and', 'other', 'figures', 'instead', 'of', 'remaining', 'fixed', 'in', 'their', 'places', 'move', 'freely', 'about', 'on', 'or', 'in', 'the', 'surface', 'but', 'without', 'the', 'power', 'of', 'rising', 'above', 'or', 'sinking', 'below', 'it', 'very', 'much', 'like', 'shadows', 'only', 'hard', 'with', 'luminous', 'edges', 'and', 'you', 'will', 'then', 'have', 'a', 'pretty', 'correct', 'notion', 'of', 'my', 'country', 'and', 'countrymen', 'alas', 'a', 'few', 'years', 'ago', 'i', 'should', 'have', 'said', 'my', 'universe', 'but', 'now', 'my', 'mind', 'has', 'been', 'opened', 'to', 'higher', 'views', 'of', 'things', 'in', 'such', 'a', 'country', 'you', 'will', 'perceive', 'at', 'once', 'that', 'it', 'is', 'impossible', 'that', 'there', 'should', 'be', 'anything', 'of', 'what', 'you', 'call', 'a', 'solid', 'kind', 'but', 'i', 'dare', 'say', 'you', 'will', 'suppose', 'that', 'we', 'could', 'at', 'least', 'distinguish', 'by', 'sight', 'the', 'triangles', 'squares', 'and', 'other', 'figures', 'moving', 'about', 'as', 'i', 'have', 'described', 'them', 'on', 'the', 'contrary', 'we', 'could', 'see', 'nothing', 'of', 'the', 'kind', 'not', 'at', 'least', 'so', 'as', 'to', 'distinguish', 'one', 'figure', 'from', 'another', 'nothing', 'was', 'visible', 'nor', 'could', 'be', 'visible', 'to', 'us', 'except', 'straight', 'lines', 'and', 'the', 'necessity', 'of', 'this', 'i', 'will', 'speedily', 'demonstrate']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "def read_text_file(file_path):\n",
    "    f = open(file_path, \"r\")\n",
    "    text = f.read()\n",
    "    return text\n",
    "\n",
    "def split_text(text):\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;]\", text.lower())\n",
    "    punctuation_list =  list(string.punctuation)\n",
    "    w_clean = list()\n",
    "    \n",
    "    for word in words:\n",
    "        new_word = \"\"\n",
    "        for char in word:\n",
    "            if char[0] and char[-1] not in punctuation_list:\n",
    "                new_word += char\n",
    "            \n",
    "        if len(new_word) > 0:\n",
    "            w_clean.append(new_word)\n",
    "        \n",
    "    return w_clean\n",
    " \n",
    "text = read_text_file(\"FileIO-DataFiles/flatland01.txt\")\n",
    "words = split_text(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Exclude Stop Words\n",
    "When searching or indexing text content (such as web pages or large documents), we typically want to exclude frequently-used words like \"the,\" \"a,\" or \"and\" so that the search or analysis includes only the words that are more likely to produce meaningful results. We use the term \"stop words\" to reference this collection of words.\n",
    "\n",
    "Because this is a common task when working with text, Python has an nltk module that includes stop words for a variety of languages. We can use this module to remove stop words from text we want to search or analyze.\n",
    "\n",
    "You may need to download extra parts of this module. To do this, run the following snippet in a cell by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Veronica\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Veronica\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    " \n",
    "nltk.download('stopwords')\n",
    " \n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that takes as input a list of words and removes all stop words. The basic steps of importing the stopwords module are provided for you, but you may find it useful to do more research on stop words before completing this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flatland', 'part', '1', 'this', 'world', 'section', '1', 'of', 'the', 'nature', 'of', 'flatland', 'i', 'call', 'our', 'world', 'flatland', 'not', 'because', 'we', 'call', 'it', 'so', 'but', 'to', 'make', 'its', 'nature', 'clearer', 'to', 'you', 'my', 'happy', 'readers', 'who', 'are', 'privileged', 'to', 'live', 'in', 'space', 'imagine', 'a', 'vast', 'sheet', 'of', 'paper', 'on', 'which', 'straight', 'lines', 'triangles', 'squares', 'pentagons', 'hexagons', 'and', 'other', 'figures', 'instead', 'of', 'remaining', 'fixed', 'in', 'their', 'places', 'move', 'freely', 'about', 'on', 'or', 'in', 'the', 'surface', 'but', 'without', 'the', 'power', 'of', 'rising', 'above', 'or', 'sinking', 'below', 'it', 'very', 'much', 'like', 'shadows', 'only', 'hard', 'with', 'luminous', 'edges', 'and', 'you', 'will', 'then', 'have', 'a', 'pretty', 'correct', 'notion', 'of', 'my', 'country', 'and', 'countrymen', 'alas', 'a', 'few', 'years', 'ago', 'i', 'should', 'have', 'said', 'my', 'universe', 'but', 'now', 'my', 'mind', 'has', 'been', 'opened', 'to', 'higher', 'views', 'of', 'things', 'in', 'such', 'a', 'country', 'you', 'will', 'perceive', 'at', 'once', 'that', 'it', 'is', 'impossible', 'that', 'there', 'should', 'be', 'anything', 'of', 'what', 'you', 'call', 'a', 'solid', 'kind', 'but', 'i', 'dare', 'say', 'you', 'will', 'suppose', 'that', 'we', 'could', 'at', 'least', 'distinguish', 'by', 'sight', 'the', 'triangles', 'squares', 'and', 'other', 'figures', 'moving', 'about', 'as', 'i', 'have', 'described', 'them', 'on', 'the', 'contrary', 'we', 'could', 'see', 'nothing', 'of', 'the', 'kind', 'not', 'at', 'least', 'so', 'as', 'to', 'distinguish', 'one', 'figure', 'from', 'another', 'nothing', 'was', 'visible', 'nor', 'could', 'be', 'visible', 'to', 'us', 'except', 'straight', 'lines', 'and', 'the', 'necessity', 'of', 'this', 'i', 'will', 'speedily', 'demonstrate']\n",
      "flatland\n",
      "part\n",
      "1\n",
      "this\n",
      "world\n",
      "section\n",
      "1\n",
      "of\n",
      "the\n",
      "nature\n",
      "of\n",
      "flatland\n",
      "i\n",
      "call\n",
      "our\n",
      "world\n",
      "flatland\n",
      "not\n",
      "because\n",
      "we\n",
      "call\n",
      "it\n",
      "so\n",
      "but\n",
      "to\n",
      "make\n",
      "its\n",
      "nature\n",
      "clearer\n",
      "to\n",
      "you\n",
      "my\n",
      "happy\n",
      "readers\n",
      "who\n",
      "are\n",
      "privileged\n",
      "to\n",
      "live\n",
      "in\n",
      "space\n",
      "imagine\n",
      "a\n",
      "vast\n",
      "sheet\n",
      "of\n",
      "paper\n",
      "on\n",
      "which\n",
      "straight\n",
      "lines\n",
      "triangles\n",
      "squares\n",
      "pentagons\n",
      "hexagons\n",
      "and\n",
      "other\n",
      "figures\n",
      "instead\n",
      "of\n",
      "remaining\n",
      "fixed\n",
      "in\n",
      "their\n",
      "places\n",
      "move\n",
      "freely\n",
      "about\n",
      "on\n",
      "or\n",
      "in\n",
      "the\n",
      "surface\n",
      "but\n",
      "without\n",
      "the\n",
      "power\n",
      "of\n",
      "rising\n",
      "above\n",
      "or\n",
      "sinking\n",
      "below\n",
      "it\n",
      "very\n",
      "much\n",
      "like\n",
      "shadows\n",
      "only\n",
      "hard\n",
      "with\n",
      "luminous\n",
      "edges\n",
      "and\n",
      "you\n",
      "will\n",
      "then\n",
      "have\n",
      "a\n",
      "pretty\n",
      "correct\n",
      "notion\n",
      "of\n",
      "my\n",
      "country\n",
      "and\n",
      "countrymen\n",
      "alas\n",
      "a\n",
      "few\n",
      "years\n",
      "ago\n",
      "i\n",
      "should\n",
      "have\n",
      "said\n",
      "my\n",
      "universe\n",
      "but\n",
      "now\n",
      "my\n",
      "mind\n",
      "has\n",
      "been\n",
      "opened\n",
      "to\n",
      "higher\n",
      "views\n",
      "of\n",
      "things\n",
      "in\n",
      "such\n",
      "a\n",
      "country\n",
      "you\n",
      "will\n",
      "perceive\n",
      "at\n",
      "once\n",
      "that\n",
      "it\n",
      "is\n",
      "impossible\n",
      "that\n",
      "there\n",
      "should\n",
      "be\n",
      "anything\n",
      "of\n",
      "what\n",
      "you\n",
      "call\n",
      "a\n",
      "solid\n",
      "kind\n",
      "but\n",
      "i\n",
      "dare\n",
      "say\n",
      "you\n",
      "will\n",
      "suppose\n",
      "that\n",
      "we\n",
      "could\n",
      "at\n",
      "least\n",
      "distinguish\n",
      "by\n",
      "sight\n",
      "the\n",
      "triangles\n",
      "squares\n",
      "and\n",
      "other\n",
      "figures\n",
      "moving\n",
      "about\n",
      "as\n",
      "i\n",
      "have\n",
      "described\n",
      "them\n",
      "on\n",
      "the\n",
      "contrary\n",
      "we\n",
      "could\n",
      "see\n",
      "nothing\n",
      "of\n",
      "the\n",
      "kind\n",
      "not\n",
      "at\n",
      "least\n",
      "so\n",
      "as\n",
      "to\n",
      "distinguish\n",
      "one\n",
      "figure\n",
      "from\n",
      "another\n",
      "nothing\n",
      "was\n",
      "visible\n",
      "nor\n",
      "could\n",
      "be\n",
      "visible\n",
      "to\n",
      "us\n",
      "except\n",
      "straight\n",
      "lines\n",
      "and\n",
      "the\n",
      "necessity\n",
      "of\n",
      "this\n",
      "i\n",
      "will\n",
      "speedily\n",
      "demonstrate\n",
      "['flatland', 'part', '1', 'world', 'section', '1', 'nature', 'flatland', 'call', 'world', 'flatland', 'call', 'make', 'nature', 'clearer', 'happy', 'readers', 'privileged', 'live', 'space', 'imagine', 'vast', 'sheet', 'paper', 'straight', 'lines', 'triangles', 'squares', 'pentagons', 'hexagons', 'figures', 'instead', 'remaining', 'fixed', 'places', 'move', 'freely', 'surface', 'without', 'power', 'rising', 'sinking', 'much', 'like', 'shadows', 'hard', 'luminous', 'edges', 'pretty', 'correct', 'notion', 'country', 'countrymen', 'alas', 'years', 'ago', 'said', 'universe', 'mind', 'opened', 'higher', 'views', 'things', 'country', 'perceive', 'impossible', 'anything', 'call', 'solid', 'kind', 'dare', 'say', 'suppose', 'could', 'least', 'distinguish', 'sight', 'triangles', 'squares', 'figures', 'moving', 'described', 'contrary', 'could', 'see', 'nothing', 'kind', 'least', 'distinguish', 'one', 'figure', 'another', 'nothing', 'visible', 'could', 'visible', 'us', 'except', 'straight', 'lines', 'necessity', 'speedily', 'demonstrate']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "import re\n",
    "import string\n",
    "def read_text_file(file_path):\n",
    "    f = open(file_path, \"r\")\n",
    "    text = f.read()\n",
    "    return text\n",
    "\n",
    "def split_text(text):\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;]\", text.lower())\n",
    "    punctuation_list =  list(string.punctuation)\n",
    "    w_clean = list()\n",
    "    \n",
    "    for word in words:\n",
    "        new_word = \"\"\n",
    "        for char in word:\n",
    "            if char[0] and char[-1] not in punctuation_list:\n",
    "                new_word += char\n",
    "            \n",
    "        if len(new_word) > 0:\n",
    "            w_clean.append(new_word)\n",
    "        \n",
    "    return w_clean\n",
    " \n",
    "def remove_stop_words(words,stop_words):\n",
    "    words_clean = list()\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            words_clean.append(word)\n",
    "        \n",
    "    return words_clean\n",
    " \n",
    "text = read_text_file(\"FileIO-DataFiles/flatland01.txt\")\n",
    "words = split_text(text)\n",
    "print(words)\n",
    "words_clean = remove_stop_words(words,stop_words)\n",
    "print(words_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Lemmatize the Words\n",
    "We can also use the nltk module to lemmatize words in a text file. The term lemmatize refers to the process of identifying words that are inflected versions of the same stem word, so that only the stem word is included in the analysis.\n",
    "\n",
    "For example, each of the following phrases includes an inflected form of the stem word \"walk\":\n",
    "\n",
    "I walked to the coffee shop last night.\n",
    "Helen regularly walks her dog in the evening.\n",
    "They saw the boys walking toward the house.\n",
    "A strict textual analysis would count each of these as a separate word, but they are all actually different forms of the same stem word, \"walk.\" Lemmatizing the words reduces the number of words that a process must analyze, making the process more efficient and the results more meaningful.\n",
    "\n",
    "The following code imports WordNetLemmatizer from the nltk.stem module and creates a lemmatizer. We can then use the lemmatizer to identify the lemma (or root form) of an inflected word, as shown in the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "priorities\n",
      "priority\n"
     ]
    }
   ],
   "source": [
    "# example code\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() \n",
    " \n",
    "word = \"priorities\"\n",
    "word_lemmatized =  lemmatizer.lemmatize(word)\n",
    "print(word) #original word\n",
    "print(word_lemmatized) #lemmatized word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this code as a starting point, create a function to lemmatize each word in a list of words produced in the previous step of this activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flatland', 'part', '1', 'this', 'world', 'section', '1', 'of', 'the', 'nature', 'of', 'flatland', 'i', 'call', 'our', 'world', 'flatland', 'not', 'because', 'we', 'call', 'it', 'so', 'but', 'to', 'make', 'its', 'nature', 'clearer', 'to', 'you', 'my', 'happy', 'readers', 'who', 'are', 'privileged', 'to', 'live', 'in', 'space', 'imagine', 'a', 'vast', 'sheet', 'of', 'paper', 'on', 'which', 'straight', 'lines', 'triangles', 'squares', 'pentagons', 'hexagons', 'and', 'other', 'figures', 'instead', 'of', 'remaining', 'fixed', 'in', 'their', 'places', 'move', 'freely', 'about', 'on', 'or', 'in', 'the', 'surface', 'but', 'without', 'the', 'power', 'of', 'rising', 'above', 'or', 'sinking', 'below', 'it', 'very', 'much', 'like', 'shadows', 'only', 'hard', 'with', 'luminous', 'edges', 'and', 'you', 'will', 'then', 'have', 'a', 'pretty', 'correct', 'notion', 'of', 'my', 'country', 'and', 'countrymen', 'alas', 'a', 'few', 'years', 'ago', 'i', 'should', 'have', 'said', 'my', 'universe', 'but', 'now', 'my', 'mind', 'has', 'been', 'opened', 'to', 'higher', 'views', 'of', 'things', 'in', 'such', 'a', 'country', 'you', 'will', 'perceive', 'at', 'once', 'that', 'it', 'is', 'impossible', 'that', 'there', 'should', 'be', 'anything', 'of', 'what', 'you', 'call', 'a', 'solid', 'kind', 'but', 'i', 'dare', 'say', 'you', 'will', 'suppose', 'that', 'we', 'could', 'at', 'least', 'distinguish', 'by', 'sight', 'the', 'triangles', 'squares', 'and', 'other', 'figures', 'moving', 'about', 'as', 'i', 'have', 'described', 'them', 'on', 'the', 'contrary', 'we', 'could', 'see', 'nothing', 'of', 'the', 'kind', 'not', 'at', 'least', 'so', 'as', 'to', 'distinguish', 'one', 'figure', 'from', 'another', 'nothing', 'was', 'visible', 'nor', 'could', 'be', 'visible', 'to', 'us', 'except', 'straight', 'lines', 'and', 'the', 'necessity', 'of', 'this', 'i', 'will', 'speedily', 'demonstrate']\n",
      "\n",
      "\n",
      "['flatland', 'part', '1', 'world', 'section', '1', 'nature', 'flatland', 'call', 'world', 'flatland', 'call', 'make', 'nature', 'clearer', 'happy', 'readers', 'privileged', 'live', 'space', 'imagine', 'vast', 'sheet', 'paper', 'straight', 'lines', 'triangles', 'squares', 'pentagons', 'hexagons', 'figures', 'instead', 'remaining', 'fixed', 'places', 'move', 'freely', 'surface', 'without', 'power', 'rising', 'sinking', 'much', 'like', 'shadows', 'hard', 'luminous', 'edges', 'pretty', 'correct', 'notion', 'country', 'countrymen', 'alas', 'years', 'ago', 'said', 'universe', 'mind', 'opened', 'higher', 'views', 'things', 'country', 'perceive', 'impossible', 'anything', 'call', 'solid', 'kind', 'dare', 'say', 'suppose', 'could', 'least', 'distinguish', 'sight', 'triangles', 'squares', 'figures', 'moving', 'described', 'contrary', 'could', 'see', 'nothing', 'kind', 'least', 'distinguish', 'one', 'figure', 'another', 'nothing', 'visible', 'could', 'visible', 'us', 'except', 'straight', 'lines', 'necessity', 'speedily', 'demonstrate']\n",
      "\n",
      "\n",
      "['flatland', 'part', '1', 'world', 'section', '1', 'nature', 'flatland', 'call', 'world', 'flatland', 'call', 'make', 'nature', 'clearer', 'happy', 'reader', 'privileged', 'live', 'space', 'imagine', 'vast', 'sheet', 'paper', 'straight', 'line', 'triangle', 'square', 'pentagon', 'hexagon', 'figure', 'instead', 'remaining', 'fixed', 'place', 'move', 'freely', 'surface', 'without', 'power', 'rising', 'sinking', 'much', 'like', 'shadow', 'hard', 'luminous', 'edge', 'pretty', 'correct', 'notion', 'country', 'countryman', 'ala', 'year', 'ago', 'said', 'universe', 'mind', 'opened', 'higher', 'view', 'thing', 'country', 'perceive', 'impossible', 'anything', 'call', 'solid', 'kind', 'dare', 'say', 'suppose', 'could', 'least', 'distinguish', 'sight', 'triangle', 'square', 'figure', 'moving', 'described', 'contrary', 'could', 'see', 'nothing', 'kind', 'least', 'distinguish', 'one', 'figure', 'another', 'nothing', 'visible', 'could', 'visible', 'u', 'except', 'straight', 'line', 'necessity', 'speedily', 'demonstrate']\n"
     ]
    }
   ],
   "source": [
    "# use this cell to complete the activity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "import re\n",
    "import string\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    f = open(file_path, \"r\")\n",
    "    text = f.read()\n",
    "    return text\n",
    "\n",
    "def split_text(text):\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;]\", text.lower())\n",
    "    punctuation_list =  list(string.punctuation)\n",
    "    w_clean = list()\n",
    "    \n",
    "    for word in words:\n",
    "        new_word = \"\"\n",
    "        for char in word:\n",
    "            if char[0] and char[-1] not in punctuation_list:\n",
    "                new_word += char\n",
    "            \n",
    "        if len(new_word) > 0:\n",
    "            w_clean.append(new_word)\n",
    "        \n",
    "    return w_clean\n",
    " \n",
    "def remove_stop_words(words,stop_words):\n",
    "    words_clean = list()\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            words_clean.append(word)\n",
    "        \n",
    "    return words_clean\n",
    "\n",
    "def lemmatize_words(words_clean):\n",
    "    words_lemmatized = list()\n",
    "    for word in words_clean:\n",
    "        word_lemmatized =  lemmatizer.lemmatize(word)\n",
    "        words_lemmatized.append(word_lemmatized)\n",
    "    return words_lemmatized\n",
    " \n",
    "text = read_text_file(\"FileIO-DataFiles/flatland01.txt\")\n",
    "words = split_text(text)\n",
    "print(words)\n",
    "print(\"\\n\")\n",
    "words_clean = remove_stop_words(words,stop_words)\n",
    "print(words_clean)\n",
    "print(\"\\n\")\n",
    "words_lemmatized = lemmatize_words(words_clean)\n",
    "print(words_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stem 5: Count the Words\n",
    "Create a function that takes as input a list of lemmatized words and returns a dictionary that has the frequency of occurrence of each lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flatland', 'part', '1', 'this', 'world', 'section', '1', 'of', 'the', 'nature', 'of', 'flatland', 'i', 'call', 'our', 'world', 'flatland', 'not', 'because', 'we', 'call', 'it', 'so', 'but', 'to', 'make', 'its', 'nature', 'clearer', 'to', 'you', 'my', 'happy', 'readers', 'who', 'are', 'privileged', 'to', 'live', 'in', 'space', 'imagine', 'a', 'vast', 'sheet', 'of', 'paper', 'on', 'which', 'straight', 'lines', 'triangles', 'squares', 'pentagons', 'hexagons', 'and', 'other', 'figures', 'instead', 'of', 'remaining', 'fixed', 'in', 'their', 'places', 'move', 'freely', 'about', 'on', 'or', 'in', 'the', 'surface', 'but', 'without', 'the', 'power', 'of', 'rising', 'above', 'or', 'sinking', 'below', 'it', 'very', 'much', 'like', 'shadows', 'only', 'hard', 'with', 'luminous', 'edges', 'and', 'you', 'will', 'then', 'have', 'a', 'pretty', 'correct', 'notion', 'of', 'my', 'country', 'and', 'countrymen', 'alas', 'a', 'few', 'years', 'ago', 'i', 'should', 'have', 'said', 'my', 'universe', 'but', 'now', 'my', 'mind', 'has', 'been', 'opened', 'to', 'higher', 'views', 'of', 'things', 'in', 'such', 'a', 'country', 'you', 'will', 'perceive', 'at', 'once', 'that', 'it', 'is', 'impossible', 'that', 'there', 'should', 'be', 'anything', 'of', 'what', 'you', 'call', 'a', 'solid', 'kind', 'but', 'i', 'dare', 'say', 'you', 'will', 'suppose', 'that', 'we', 'could', 'at', 'least', 'distinguish', 'by', 'sight', 'the', 'triangles', 'squares', 'and', 'other', 'figures', 'moving', 'about', 'as', 'i', 'have', 'described', 'them', 'on', 'the', 'contrary', 'we', 'could', 'see', 'nothing', 'of', 'the', 'kind', 'not', 'at', 'least', 'so', 'as', 'to', 'distinguish', 'one', 'figure', 'from', 'another', 'nothing', 'was', 'visible', 'nor', 'could', 'be', 'visible', 'to', 'us', 'except', 'straight', 'lines', 'and', 'the', 'necessity', 'of', 'this', 'i', 'will', 'speedily', 'demonstrate']\n",
      "\n",
      "\n",
      "['flatland', 'part', '1', 'world', 'section', '1', 'nature', 'flatland', 'call', 'world', 'flatland', 'call', 'make', 'nature', 'clearer', 'happy', 'readers', 'privileged', 'live', 'space', 'imagine', 'vast', 'sheet', 'paper', 'straight', 'lines', 'triangles', 'squares', 'pentagons', 'hexagons', 'figures', 'instead', 'remaining', 'fixed', 'places', 'move', 'freely', 'surface', 'without', 'power', 'rising', 'sinking', 'much', 'like', 'shadows', 'hard', 'luminous', 'edges', 'pretty', 'correct', 'notion', 'country', 'countrymen', 'alas', 'years', 'ago', 'said', 'universe', 'mind', 'opened', 'higher', 'views', 'things', 'country', 'perceive', 'impossible', 'anything', 'call', 'solid', 'kind', 'dare', 'say', 'suppose', 'could', 'least', 'distinguish', 'sight', 'triangles', 'squares', 'figures', 'moving', 'described', 'contrary', 'could', 'see', 'nothing', 'kind', 'least', 'distinguish', 'one', 'figure', 'another', 'nothing', 'visible', 'could', 'visible', 'us', 'except', 'straight', 'lines', 'necessity', 'speedily', 'demonstrate']\n",
      "\n",
      "\n",
      "['flatland', 'part', '1', 'world', 'section', '1', 'nature', 'flatland', 'call', 'world', 'flatland', 'call', 'make', 'nature', 'clearer', 'happy', 'reader', 'privileged', 'live', 'space', 'imagine', 'vast', 'sheet', 'paper', 'straight', 'line', 'triangle', 'square', 'pentagon', 'hexagon', 'figure', 'instead', 'remaining', 'fixed', 'place', 'move', 'freely', 'surface', 'without', 'power', 'rising', 'sinking', 'much', 'like', 'shadow', 'hard', 'luminous', 'edge', 'pretty', 'correct', 'notion', 'country', 'countryman', 'ala', 'year', 'ago', 'said', 'universe', 'mind', 'opened', 'higher', 'view', 'thing', 'country', 'perceive', 'impossible', 'anything', 'call', 'solid', 'kind', 'dare', 'say', 'suppose', 'could', 'least', 'distinguish', 'sight', 'triangle', 'square', 'figure', 'moving', 'described', 'contrary', 'could', 'see', 'nothing', 'kind', 'least', 'distinguish', 'one', 'figure', 'another', 'nothing', 'visible', 'could', 'visible', 'u', 'except', 'straight', 'line', 'necessity', 'speedily', 'demonstrate']\n",
      "\n",
      "\n",
      "<class 'dict'>\n",
      "\n",
      "\n",
      "{'flatland': 3, 'part': 1, '1': 2, 'world': 2, 'section': 1, 'nature': 2, 'call': 3, 'make': 1, 'clearer': 1, 'happy': 1, 'reader': 0, 'privileged': 1, 'live': 1, 'space': 1, 'imagine': 1, 'vast': 1, 'sheet': 1, 'paper': 1, 'straight': 2, 'line': 0, 'triangle': 0, 'square': 0, 'pentagon': 0, 'hexagon': 0, 'figure': 1, 'instead': 1, 'remaining': 1, 'fixed': 1, 'place': 0, 'move': 1, 'freely': 1, 'surface': 1, 'without': 1, 'power': 1, 'rising': 1, 'sinking': 1, 'much': 1, 'like': 1, 'shadow': 0, 'hard': 1, 'luminous': 1, 'edge': 0, 'pretty': 1, 'correct': 1, 'notion': 1, 'country': 2, 'countryman': 0, 'ala': 0, 'year': 0, 'ago': 1, 'said': 1, 'universe': 1, 'mind': 1, 'opened': 1, 'higher': 1, 'view': 0, 'thing': 0, 'perceive': 1, 'impossible': 1, 'anything': 1, 'solid': 1, 'kind': 2, 'dare': 1, 'say': 1, 'suppose': 1, 'could': 3, 'least': 2, 'distinguish': 2, 'sight': 1, 'moving': 1, 'described': 1, 'contrary': 1, 'see': 1, 'nothing': 2, 'one': 1, 'another': 1, 'visible': 2, 'u': 0, 'except': 1, 'necessity': 1, 'speedily': 1, 'demonstrate': 1}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "import re\n",
    "import string\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    f = open(file_path, \"r\")\n",
    "    text = f.read()\n",
    "    return text\n",
    "\n",
    "def split_text(text):\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;]\", text.lower())\n",
    "    punctuation_list =  list(string.punctuation)\n",
    "    w_clean = list()\n",
    "    \n",
    "    for word in words:\n",
    "        new_word = \"\"\n",
    "        for char in word:\n",
    "            if char[0] and char[-1] not in punctuation_list:\n",
    "                new_word += char\n",
    "            \n",
    "        if len(new_word) > 0:\n",
    "            w_clean.append(new_word)\n",
    "        \n",
    "    return w_clean\n",
    " \n",
    "def remove_stop_words(words,stop_words):\n",
    "    words_clean = list()\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            words_clean.append(word)\n",
    "        \n",
    "    return words_clean\n",
    "\n",
    "def lemmatize_words(words_clean):\n",
    "    words_lemmatized = list()\n",
    "    for word in words_clean:\n",
    "        word_lemmatized =  lemmatizer.lemmatize(word)\n",
    "        words_lemmatized.append(word_lemmatized)\n",
    "    return words_lemmatized\n",
    "def compute_frequency_words(words_lemmatized):\n",
    "    word_freq = dict()\n",
    "\n",
    "    for word in words_lemmatized:\n",
    "        word_freq[word] = (words.count(word))\n",
    "    \n",
    "    return word_freq\n",
    "\n",
    "text = read_text_file(\"FileIO-DataFiles/flatland01.txt\")\n",
    "words = split_text(text)\n",
    "print(words)\n",
    "print(\"\\n\")\n",
    "words_clean = remove_stop_words(words,stop_words)\n",
    "print(words_clean)\n",
    "print(\"\\n\")\n",
    "words_lemmatized = lemmatize_words(words_clean)\n",
    "print(words_lemmatized)\n",
    "print(\"\\n\")\n",
    "words_frequency = compute_frequency_words(words_lemmatized)\n",
    "print(type(words_frequency)) #should print dict\n",
    "print(\"\\n\")\n",
    "print(words_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Export the Results to JSON\n",
    "Create a function that takes as input a dictionary where the key is a word and the value is the frequency of occurrence of that word in an input text.\n",
    "\n",
    "The function should store the dictionary in a JSON file named words_frequency.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flatland', 'part', '1', 'this', 'world', 'section', '1', 'of', 'the', 'nature', 'of', 'flatland', 'i', 'call', 'our', 'world', 'flatland', 'not', 'because', 'we', 'call', 'it', 'so', 'but', 'to', 'make', 'its', 'nature', 'clearer', 'to', 'you', 'my', 'happy', 'readers', 'who', 'are', 'privileged', 'to', 'live', 'in', 'space', 'imagine', 'a', 'vast', 'sheet', 'of', 'paper', 'on', 'which', 'straight', 'lines', 'triangles', 'squares', 'pentagons', 'hexagons', 'and', 'other', 'figures', 'instead', 'of', 'remaining', 'fixed', 'in', 'their', 'places', 'move', 'freely', 'about', 'on', 'or', 'in', 'the', 'surface', 'but', 'without', 'the', 'power', 'of', 'rising', 'above', 'or', 'sinking', 'below', 'it', 'very', 'much', 'like', 'shadows', 'only', 'hard', 'with', 'luminous', 'edges', 'and', 'you', 'will', 'then', 'have', 'a', 'pretty', 'correct', 'notion', 'of', 'my', 'country', 'and', 'countrymen', 'alas', 'a', 'few', 'years', 'ago', 'i', 'should', 'have', 'said', 'my', 'universe', 'but', 'now', 'my', 'mind', 'has', 'been', 'opened', 'to', 'higher', 'views', 'of', 'things', 'in', 'such', 'a', 'country', 'you', 'will', 'perceive', 'at', 'once', 'that', 'it', 'is', 'impossible', 'that', 'there', 'should', 'be', 'anything', 'of', 'what', 'you', 'call', 'a', 'solid', 'kind', 'but', 'i', 'dare', 'say', 'you', 'will', 'suppose', 'that', 'we', 'could', 'at', 'least', 'distinguish', 'by', 'sight', 'the', 'triangles', 'squares', 'and', 'other', 'figures', 'moving', 'about', 'as', 'i', 'have', 'described', 'them', 'on', 'the', 'contrary', 'we', 'could', 'see', 'nothing', 'of', 'the', 'kind', 'not', 'at', 'least', 'so', 'as', 'to', 'distinguish', 'one', 'figure', 'from', 'another', 'nothing', 'was', 'visible', 'nor', 'could', 'be', 'visible', 'to', 'us', 'except', 'straight', 'lines', 'and', 'the', 'necessity', 'of', 'this', 'i', 'will', 'speedily', 'demonstrate']\n",
      "\n",
      "\n",
      "['flatland', 'part', '1', 'world', 'section', '1', 'nature', 'flatland', 'call', 'world', 'flatland', 'call', 'make', 'nature', 'clearer', 'happy', 'readers', 'privileged', 'live', 'space', 'imagine', 'vast', 'sheet', 'paper', 'straight', 'lines', 'triangles', 'squares', 'pentagons', 'hexagons', 'figures', 'instead', 'remaining', 'fixed', 'places', 'move', 'freely', 'surface', 'without', 'power', 'rising', 'sinking', 'much', 'like', 'shadows', 'hard', 'luminous', 'edges', 'pretty', 'correct', 'notion', 'country', 'countrymen', 'alas', 'years', 'ago', 'said', 'universe', 'mind', 'opened', 'higher', 'views', 'things', 'country', 'perceive', 'impossible', 'anything', 'call', 'solid', 'kind', 'dare', 'say', 'suppose', 'could', 'least', 'distinguish', 'sight', 'triangles', 'squares', 'figures', 'moving', 'described', 'contrary', 'could', 'see', 'nothing', 'kind', 'least', 'distinguish', 'one', 'figure', 'another', 'nothing', 'visible', 'could', 'visible', 'us', 'except', 'straight', 'lines', 'necessity', 'speedily', 'demonstrate']\n",
      "\n",
      "\n",
      "['flatland', 'part', '1', 'world', 'section', '1', 'nature', 'flatland', 'call', 'world', 'flatland', 'call', 'make', 'nature', 'clearer', 'happy', 'reader', 'privileged', 'live', 'space', 'imagine', 'vast', 'sheet', 'paper', 'straight', 'line', 'triangle', 'square', 'pentagon', 'hexagon', 'figure', 'instead', 'remaining', 'fixed', 'place', 'move', 'freely', 'surface', 'without', 'power', 'rising', 'sinking', 'much', 'like', 'shadow', 'hard', 'luminous', 'edge', 'pretty', 'correct', 'notion', 'country', 'countryman', 'ala', 'year', 'ago', 'said', 'universe', 'mind', 'opened', 'higher', 'view', 'thing', 'country', 'perceive', 'impossible', 'anything', 'call', 'solid', 'kind', 'dare', 'say', 'suppose', 'could', 'least', 'distinguish', 'sight', 'triangle', 'square', 'figure', 'moving', 'described', 'contrary', 'could', 'see', 'nothing', 'kind', 'least', 'distinguish', 'one', 'figure', 'another', 'nothing', 'visible', 'could', 'visible', 'u', 'except', 'straight', 'line', 'necessity', 'speedily', 'demonstrate']\n",
      "\n",
      "\n",
      "<class 'dict'>\n",
      "\n",
      "\n",
      "{'flatland': 3, 'part': 1, '1': 2, 'world': 2, 'section': 1, 'nature': 2, 'call': 3, 'make': 1, 'clearer': 1, 'happy': 1, 'reader': 0, 'privileged': 1, 'live': 1, 'space': 1, 'imagine': 1, 'vast': 1, 'sheet': 1, 'paper': 1, 'straight': 2, 'line': 0, 'triangle': 0, 'square': 0, 'pentagon': 0, 'hexagon': 0, 'figure': 1, 'instead': 1, 'remaining': 1, 'fixed': 1, 'place': 0, 'move': 1, 'freely': 1, 'surface': 1, 'without': 1, 'power': 1, 'rising': 1, 'sinking': 1, 'much': 1, 'like': 1, 'shadow': 0, 'hard': 1, 'luminous': 1, 'edge': 0, 'pretty': 1, 'correct': 1, 'notion': 1, 'country': 2, 'countryman': 0, 'ala': 0, 'year': 0, 'ago': 1, 'said': 1, 'universe': 1, 'mind': 1, 'opened': 1, 'higher': 1, 'view': 0, 'thing': 0, 'perceive': 1, 'impossible': 1, 'anything': 1, 'solid': 1, 'kind': 2, 'dare': 1, 'say': 1, 'suppose': 1, 'could': 3, 'least': 2, 'distinguish': 2, 'sight': 1, 'moving': 1, 'described': 1, 'contrary': 1, 'see': 1, 'nothing': 2, 'one': 1, 'another': 1, 'visible': 2, 'u': 0, 'except': 1, 'necessity': 1, 'speedily': 1, 'demonstrate': 1}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "import re\n",
    "import string\n",
    "import json\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    f = open(file_path, \"r\")\n",
    "    text = f.read()\n",
    "    return text\n",
    "\n",
    "def split_text(text):\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;]\", text.lower())\n",
    "    punctuation_list =  list(string.punctuation)\n",
    "    w_clean = list()\n",
    "    \n",
    "    for word in words:\n",
    "        new_word = \"\"\n",
    "        for char in word:\n",
    "            if char[0] and char[-1] not in punctuation_list:\n",
    "                new_word += char\n",
    "            \n",
    "        if len(new_word) > 0:\n",
    "            w_clean.append(new_word)\n",
    "        \n",
    "    return w_clean\n",
    " \n",
    "def remove_stop_words(words,stop_words):\n",
    "    words_clean = list()\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            words_clean.append(word)\n",
    "        \n",
    "    return words_clean\n",
    "\n",
    "def lemmatize_words(words_clean):\n",
    "    words_lemmatized = list()\n",
    "    for word in words_clean:\n",
    "        word_lemmatized =  lemmatizer.lemmatize(word)\n",
    "        words_lemmatized.append(word_lemmatized)\n",
    "    return words_lemmatized\n",
    "def compute_frequency_words(words_lemmatized):\n",
    "    word_freq = dict()\n",
    "\n",
    "    for word in words_lemmatized:\n",
    "        word_freq[word] = (words.count(word))\n",
    "    \n",
    "    return word_freq\n",
    "\n",
    "def save_words_frequency(words_frequency,file_path=\"FileIO-DataFiles/words_frequency.json\"):\n",
    "    with open(file_path, 'w') as outfile:  \n",
    "        json.dump(words_frequency, outfile)\n",
    "\n",
    "text = read_text_file(\"FileIO-DataFiles/flatland01.txt\")\n",
    "words = split_text(text)\n",
    "print(words)\n",
    "print(\"\\n\")\n",
    "words_clean = remove_stop_words(words,stop_words)\n",
    "print(words_clean)\n",
    "print(\"\\n\")\n",
    "words_lemmatized = lemmatize_words(words_clean)\n",
    "print(words_lemmatized)\n",
    "print(\"\\n\")\n",
    "words_frequency = compute_frequency_words(words_lemmatized)\n",
    "print(type(words_frequency)) #should print dict\n",
    "print(\"\\n\")\n",
    "print(words_frequency)\n",
    "save_words_frequency(words_frequency,\"FileIO-DataFiles/words_frequency.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Combine All Steps in a Single Program\n",
    "Using the skeleton below, combine all of the previous steps into a single script that will perform the following steps:\n",
    "\n",
    "Convert a text file to a string.\n",
    "Split the string into words, excluding punctuation marks.\n",
    "Remove stop words from the list of strings.\n",
    "Lemmatize the words in the list so that all words are stem words.\n",
    "Count the frequency of each stem word and store the results in a dictionary.\n",
    "Convert the dictionary to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "import re\n",
    "import string\n",
    "import json\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    f = open(file_path, \"r\")\n",
    "    text = f.read()\n",
    "    return text\n",
    "\n",
    "def split_text(text):\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;]\", text.lower())\n",
    "    punctuation_list =  list(string.punctuation)\n",
    "    w_clean = list()\n",
    "    \n",
    "    for word in words:\n",
    "        new_word = \"\"\n",
    "        for char in word:\n",
    "            if char[0] and char[-1] not in punctuation_list:\n",
    "                new_word += char\n",
    "            \n",
    "        if len(new_word) > 0:\n",
    "            w_clean.append(new_word)\n",
    "        \n",
    "    return w_clean\n",
    " \n",
    "def remove_stop_words(words,stop_words):\n",
    "    words_clean = list()\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            words_clean.append(word)\n",
    "        \n",
    "    return words_clean\n",
    "\n",
    "def lemmatize_words(words_clean):\n",
    "    words_lemmatized = list()\n",
    "    for word in words_clean:\n",
    "        word_lemmatized =  lemmatizer.lemmatize(word)\n",
    "        words_lemmatized.append(word_lemmatized)\n",
    "    return words_lemmatized\n",
    "def compute_frequency_words(words_lemmatized):\n",
    "    word_freq = dict()\n",
    "\n",
    "    for word in words_lemmatized:\n",
    "        word_freq[word] = (words.count(word))\n",
    "    \n",
    "    return word_freq\n",
    "\n",
    "def save_words_frequency(words_frequency,file_path=\"FileIO-DataFiles/words_frequency.json\"):\n",
    "    with open(file_path, 'w') as outfile:  \n",
    "        json.dump(words_frequency, outfile)\n",
    "\n",
    "text = read_text_file(\"FileIO-DataFiles/flatland01.txt\")\n",
    "words = split_text(text)\n",
    "words_clean = remove_stop_words(words,stop_words)\n",
    "words_lemmatized = lemmatize_words(words_clean)\n",
    "words_frequency = compute_frequency_words(words_lemmatized)\n",
    "save_words_frequency(words_frequency,\"FileIO-DataFiles/words_frequency.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements\n",
    "After completing all steps in this assignment, verify that your code meets the following requirements:\n",
    "\n",
    "Your name and a current date appear as a comment in the first line of code.\n",
    "The final version of the file successfully completes each of the following tasks:\n",
    "Convert a text file to a string.\n",
    "Split the string into words, excluding punctuation marks.\n",
    "Remove stop words from the list of strings.\n",
    "Lemmatize the words in the list so that all words are stem words.\n",
    "Count the frequency of each stem word and store the results in a dictionary.\n",
    "Convert the dictionary to a JSON file.\n",
    "Include appropriate exception handling for predictable errors such as missing files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
